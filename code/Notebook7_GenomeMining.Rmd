---
title: "Notebook4_ToxinGene_Readmap_K22_23"
output: html_document
date: "2023-12-31"
---

```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(here)
library(vroom)
library(ggmap)
library(googledrive)
library(readxl)
library(Biostrings)
library(lubridate)
library(ggplot2)
library(ggpmisc)
library(Rsamtools)
library(ggnewscale)
library(cowplot)
library(furrr)
library(RColorBrewer)
library(pheatmap)
library(ggfortify)
library(gridExtra)
knitr::opts_knit$set(root.dir = here::here("")) 
```

NOTEBOOK BREAKDOWN

#####################################################################Creating BGC Database from MAGs curated from Field Data
1. Pull all high and medium quality cyanobacterial MAGs for antiSMASH
2. Download antismash and multimash
3. Drep bins, Process bins using bakta GLAMR rule to have inputs to multimash, also ran drep on these
4. Run multimash on the gbff files output from bakta
4. Read in multimash results to pull CSV together for pulling sequence database
5. Pull all BGCs into one fasta file and then cluster them to dereplicate the database 
6. Run bigscape on new antismash results and visualize--Making Figure S2
7. Extract gbk info from antismash output of dereplicated bgcs for annotation offline
8. Readmap whole BGCs to reads to see what data looks like --> decide if need to refine bgcs to core and additional only
9. Process read mapping of whole BGCs 
10. Visualize read mapping of whole BGCs
#####################################################################Creating Database for Curated BGCs from MAGs from the Field Data
11. Manually curate BGC database and add annotations to them
12. Pull manually curated BGC database together-
#####################################################################Data processing for Figure 8 and 9 
13. Read map manually curated BGC database to reads 
14. Process read mapping 
15. Merge data and prepare data to be plotted from read mapping (merge bam and stats file with metadata)
#####################################################################Making Figure 8 and 9
16. Make pub ready figure of BGC mapping for holistic biosynthetic potential-per bin and pathway (Figure 8)
17. Make pub ready figures of BGC mapping for holistic biosynthetic potential-per station (Figure 9)
18. Make pub ready figure of BGCs ordinated by NMDS with loadings of major bloom-forming cyanobacteria abundance patterns (SI Figure 4)




#####################################################################Creating BGC Database from MAGs curated from Field Data

1. Pull all high and medium quality cyanobacterial MAGs for antiSMASH (used 70% completion, 30% contamination)
```{r}
# read in quality datasheet
kenya_quality2 = read.csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/checkm_gtdb_results_22-23-Dec-22-2023.csv", header = TRUE)

##Exclude Samples not from Lake Victoria (samp_4327, samp_4328, samp_4329, samp_4330, samp_4455, samp_4456, samp_4453)

quality_exc2 = kenya_quality2 %>% filter(!checkm_sample == "samp_4327", !checkm_sample == "samp_4328", !checkm_sample == "samp_4329", !checkm_sample == "samp_4330", !checkm_sample == "samp_4455", !checkm_sample == "samp_4456", !checkm_sample == "samp_4453")

dim(kenya_quality2)
#[1] 22073    15
dim(quality_exc2)
#[1] 17990    15

##Extract cyanobacterial bins greater than 90% completion and 10% contamination into a dataframe 
quality_exc_bins = quality_exc1 %>% filter(Phylum == "Cyanobacteria") %>% filter(checkm_completeness >= 70 & checkm_contamination <= 30)
dim(quality_exc_bins)
#[1] 749  14

###pull all bins (takes a long time to run)

# #2023 Bins
# drep_bin_paths23 <- system("find /geomicro/data2/kiledal/GLAMR/data/projects/set_57/metagenomes/samp_*/bins/bins_for_drep/ -path *.fa", intern=TRUE) %>% tibble(bin_path = .) %>% mutate(sample = bin_path %>% str_remove(".*metagenomes/") %>% str_remove("/bins.*"), bin = bin_path %>% str_remove(".*bins_for_drep/") %>% str_remove(".fa"))
# 
# 
# #2022 Bins 
# drep_bin_paths22 <- system("find /geomicro/data2/kiledal/GLAMR/data/projects/set_55/metagenomes/samp_*/bins/bins_for_drep/ -path *.fa", intern=TRUE) %>% tibble(bin_path = .) %>% mutate(sample = bin_path %>% str_remove(".*metagenomes/") %>% str_remove("/bins.*"), bin = bin_path %>% str_remove(".*bins_for_drep/") %>% str_remove(".fa"))
# 
# 
# #Combine bin list 
# drep_bins = rbind(drep_bin_paths22, drep_bin_paths23)

###put all these bins into one folder

quality_bin_path = drep_bins %>% mutate(new_path = str_glue("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/{bin}.fa")) %>% filter(bin %in% quality_exc_bins$genome)

file.symlink(quality_bin_path$bin_path, quality_bin_path$new_path)

```

2. Download multimash and antismash
```{r}
#Downloading antismash: https://github.com/zreitz/multismash/#antismash-7-installation-protocol

# Create the environment. Conda should work too, but mamba is faster
# mamba create -n antismash7 python=3.10      # Python must be v 3.9+
# conda activate antismash7

# Install dependencies
# mamba install -c bioconda hmmer2 hmmer diamond fasttree prodigal glimmerhmm

# Download and install antiSMASH v7.1
# Get different versions by changing the branch: https://github.com/antismash/antismash/branches/all
# git clone --branch 7-1-stable https://github.com/antismash/antismash.git antismash7   
# cd antismash7
# pip install -e .
#had to install pip install pandas and pip install pytest before this

#The reason antiSMASH 7 is not one-line conda installable is that it requires meme<=4.11.2, which doesn't play well with the other dependencies. You will have to install the old version of meme suite separately and direct antiSMASH to the binaries for meme and fimo.

#You can create a separate conda environment just for the old meme version, then tell antiSMASH permanently where to find the binaries using the antiSMASH config file:

# mamba create --name meme_4.11.2 -c bioconda meme=4.11.2
# mamba activate meme_4.11.2
# 
# # Permanently tell antiSMASH v7 where to find the executables
# echo "executable-paths meme=$(which meme),fimo=$(which fimo)" >> ~/.antismash7.cfg
# 
# # Return to antismash environment
# mamba activate antismash7
# 
# #Finally, download the various databases that antiSMASH requires:
# 
# download-antismash-databases
# 
# #Test your installation:
# 
# antismash --check-prereqs
# 
# #https://github.com/zreitz/multismash/
# 
# conda activate antismash7
# 
# git clone https://github.com/zreitz/multismash.git
# cd multismash
# pip install -e .
  
```

3. Drep bins, Process bins using bakta GLAMR rule to have inputs to multimash, also ran drep on these
```{r}
#running drep at 99%, ended up with 126 bins
#dRep dereplicate --S_ani 0.99 -p 12 /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99 -g /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/bins/*.fa


#running bakta 
#results found here: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/bakta_bins_99/bins

#snakefile here: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/code

##Move GBFF files to folder for genbanks to multismash

read_paths <- system("ls /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/bakta_bins/samp_*/samp_*.gbff", intern = TRUE) %>% 
  data_frame(read_path = .)  %>% 
  mutate(sample = read_path %>% 
           str_remove("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/bakta_bins/") %>% 
           str_extract("samp_\\d+_\\w+"),   
         new_path = str_glue("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/antismash7/multismash/example/genbanks_qualitydrep99/{sample}.gbff"))


file.symlink(read_paths$read_path, read_paths$new_path)

```

4. Run multimash on the gbff files output from bakta
```{r}
#conda activate antismash 7

# cd /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/antismash7/multismash/example

#Edit config-example.yaml file to be correct, change cores, in and output directories, etc. 

#Run a dry run: 
# antismash7) lnhart@cayman:/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/antismash7/multismash/example$ cd ../
# (antismash7) lnhart@cayman:/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/antismash7/multismash$ multismash example/config-example.yaml -n

#Ran into error with pulp and list_solvers, clicked on that init python file and changed to listSolvers, dry run ran fine

#Run real run:multismash example/config-example.yaml, started at 4:30pm on Jan 28, 2024 on 36 cores

#re ran multismash: multismash example/config-example.yaml --rerun-incomplete
```

4. Read in multimash results and prepare a csv for pulling these operons
```{r}
multimash_nodrep = read_tsv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/all_regions.tsv")

table(multimash_nodrep$KCB_hit)
dim(multimash_nodrep)
#[1] 1524   11

#Filter for BGCs smaller than 5000 bp

multimash_filtered = multimash_nodrep %>% mutate(bgc_length = end - start) %>% filter(bgc_length >= 5000)
dim(multimash_filtered)
#[1] 1103   12

#Make tsv for pulling results 
multimash_parse = multimash_filtered %>% select(c(start, end, file, record_id, region, product, KCB_hit))

#Make header name for the genes 
multimash_parse = multimash_parse %>% mutate(new_header = paste(file, record_id, product, sep = "_"))

#Read csv into server
write_csv(multimash_parse, "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/multimash_nodrep_parsingfile.csv")
```

5. Pull all BGCs into one fasta file and then cluster them to dereplicate the database 
```{r}
#Used parsing script found here: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/scripts/extract_bgcs_gbff.py and executed using python extract_bgcs_gbff.py

#Used this reference CSV to pull them: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/multimash_nodrep_parsingfile.csv

#Results are found here: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/references/extracted_bgcs_nodrep3.fasta

#For clustering: going to try to cluster at 95, 90, and 80 percent identity 

#mmseqs easy-cluster extracted_bgcs_nodrep3.fasta extractedbgcs_95_90 tmp --min-seq-id 0.95 -c 0.9 --cov-mode 1
#2207 rows to 617
#1103 sequences originally

#mmseqs easy-cluster extracted_bgcs_nodrep3.fasta extractedbgcs_90_90 tmp --min-seq-id 0.90 -c 0.9 --cov-mode 1
#reduced to 607 lines (almost same as 95)

#mmseqs easy-cluster extracted_bgcs_nodrep3.fasta extractedbgcs_80_80 tmp --min-seq-id 0.8 -c 0.8 --cov-mode 1
#reduced size to 500 lines  

#Carrying no with 95% ID and 90% coverage clustered DB and running in Antismash online: extractedbgcs_95_90_rep_seq.fasta

#Ran in antismash online, version 7.1.0, Feb-16-2024 
```

6. Run bigscape on new antismash results
```{r}
##Bigscape was run on all antismash results (downloaded from the online antismash submission) with the snakefile found here and code below: 

# import os
# import re
# import snakemake.io
# from glob import glob
# 
# #/geomicro/data21/lnhart/CSP22_data/config/snakemake_profiles/cayman
# 
# rule bigscape:
#     input: 
#         antismash_gbk_dir = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/extractedbgcs_95_90_rep_seq/single_gbks",
#         db = "/geomicro/data2/kiledal/GLAMR/data/reference/bigscape",
#         bigscape_repo = "/geomicro/data2/kiledal/GLAMR/data/reference/BiG-SCAPE"
#     output:
#         #dir = directory("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/bigscape/output")
#         done = touch("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/bigscape2/.done")
#     params:
#         dir = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/bigscape2/output"
#     conda: "/geomicro/data2/kiledal/GLAMR/config/conda_yaml/bigscape.yaml"
#     #singularity: "docker://eandersk/big-scape"
#     log: "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/bigscape2/logs/drep95_k22_23_bins.log"
#     #benchmark: "benchmarks/bigscape/{sample_type}-{project}.tsv"
#     resources: cpus=24, mem_mb=100000, time_min=20000
#     shell:
#         """
#         PATH={input.bigscape_repo}:$PATH
# 
#         bigscape.py \
#             --inputdir {input.antismash_gbk_dir} \
#             --outputdir {params.dir} \
#             --pfam_dir {input.db} \
#             --include_singletons \
#             --mibig \
#             --cores {resources.cpus} | tee {log}
#         """


#Results found here: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/results/bigscape2

#This folder was then downloaded onto my local computer. Htmls can visualize the output online. I gathered all the network files within output --> networks --> BGCname --> *.network file and dragged and dropped them into cytoscape. I then deleted all mibig singletons (BGC****). I then went to tools --> merge --> networks and merged all these networks together to get them into one. Next, I gathered the network_annotation_full.tsv and have added a genus column to it so i can visualize the genus of each bgc we have found based on bin annotation. Below I am getting the genera of the bins together to add to that list and then i will upload this as a table to the cytoscape merged network.

#This was used to make Figure S2. 

genomes = read_csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/genomes_genera.csv")
quality = read_csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/checkm_gtdb_results_22-23-Dec-22-2023.csv")

genomes_qual = left_join(genomes, quality, by = "genome")
dim(genomes)
#[1] 307   1
dim(genomes_qual)
#[1] 307  15

write_csv(genomes_qual, "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/quality_steps/genomes_tax.csv")
```

7. Extract gbk info from antismash output of dereplicated bgcs for annotation offline
```{r}
#I needed to extract the BGC information per gene to be able to annotate some of these gene clusters to get a better idea of biosynthetic potential in the dataset. Additionally, I wanted to refine the BGCs detected so I can readmap to a more marker gene set of genes. 

#To do this, I used this script:
#/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/scripts/parsing_gbks.py to parse the overall antismash gbk file found here:

#/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/results/antismash_drep_BGCs_9590/extractedbgcs_95_90_rep_seq/extractedbgcs_95_90_rep_seq.gbk 

#This outputted a csv file: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/drep95_antismash.csv

#I downloaded this csv onto my computer and uploaded it to a google sheet to begin manually annotating gene clusters: https://docs.google.com/spreadsheets/d/1UaUYG6ll0A2PuafYUBd1-kETdX6J724qV6qq9HPT1ug/edit#gid=217840513

```

8. Readmap whole BGCs to reads to see what data looks like --> decide if need to refine bgcs to core and additional only
```{r}
#Snakefile
#/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/Snakefile

# import os
# import re
# import snakemake.io
# from glob import glob
# 
# qc_reads = glob_wildcards("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/decon_reads/{sample}_fwd.fastq.gz").sample  
# 
# rule read_mapping_wholebgc_kenya:
#     input: 
#         f_reads = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/decon_reads/{sample}_fwd.fastq.gz",
#         r_reads = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/decon_reads/{sample}_rev.fastq.gz",
#         ref = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/extractedbgcs_90_90_rep_seq.fasta"
#     output: 
#         sam = temp("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results/{sample}_mapped.sam"),
#         temp_bam = temp("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/{sample}_mapped_temp.bam"),
#         unsorted_bam = temp("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/{sample}_mapped_unsorted.bam"),
#         bam = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results/{sample}_mapped.bam"
#     conda: "/geomicro/data21/lnhart/Projects/Changing-bloom/usgs-metagenomes/toxin-marker-readmap/minimap2.yaml"
#     log: "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results/{sample}_mapped_bam.log"
#     resources: cpus=48
#     shell: 
#         """
#         minimap2 \
#             -ax sr \
#             -t {resources.cpus} \
#             --secondary=no \
#             {input.ref} \
#             {input.f_reads} {input.r_reads} > {output.sam}
# 
#         samtools view -bS {output.sam} > {output.temp_bam}
# 
#         filterBam \
#             --in {output.temp_bam} \
#             --out {output.unsorted_bam} \
#             --minCover 80 \
#             --minId 90
# 
#             samtools sort -o {output.bam} -@ {resources.cpus} {output.unsorted_bam}
#             samtools index -@ {resources.cpus} {output.bam}
#         """
# 
# rule read_mapping_pileup_wholebgc_kenya:
#     input:
#         bam = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results/{sample}_mapped.bam",
#         ref = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/extractedbgcs_90_90_rep_seq.fasta"
#     output:
#         pileup = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results/{sample}_pileup.txt"
#     conda: "/geomicro/data21/lnhart/Projects/Changing-bloom/usgs-metagenomes/toxin-marker-readmap/minimap2.yaml"
#     log: "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results/{sample}_pileup.log"
#     resources: cpus=48
#     shell:
#         """
#         samtools mpileup -f {input.ref} -o {output.pileup} {input.bam}
#         """
# 
# rule run_read_mapping_wholebgc_kenya:
#     input:
#         expand("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results/{sample}_pileup.txt", sample = qc_reads)

#Command
#snakemake run_read_mapping_wholebgc_kenya --profile /geomicro/data21/lnhart/CSP22_data/config/snakemake_profiles/vondamm --dry-run

#Results
#/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results
```

9. Process read mapping of whole BGCs-RPKM, coverage, %id
```{r}
######1. Process toxin marker gene read mapping results: RPKM

####Preparing data for function
#% coverage is 80% and 90% identity for this data when run by minimap, can filter for whole BGC coverage in step 2 of this cell

#read in reference sequence fasta 
toxin_MAG_ref <- Biostrings::readDNAStringSet("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/extractedbgcs_90_90_rep_seq.fasta") %>%  # import reference fasta
    as.character() %>%
    data.frame(ref_base = ., seqnames = names(.)) %>%
      mutate(mag = str_extract(seqnames, pattern),  # label MAG names
           seq_length = nchar(ref_base))

#Edit sequence names to end where there is a space, not entirely sure there are spaces in these names so let it be until theres an issue 
toxin_MAG_ref$seqnames <- sub(" .*", "", toxin_MAG_ref$seqnames)

#pull sequence names into a vector 
vector = toxin_MAG_ref$seqnames
pattern <- paste(vector, collapse = "|")

ref_mag_length <- toxin_MAG_ref %>%
    group_by(seqnames) %>%
      mutate(mag_length = sum(seq_length)) %>%  # get length of each MAG
    reframe(mag = unique(seqnames),
            mag_length = unique(mag_length))

#remove seqnames from dataframe
ref_mag_length = ref_mag_length %>% select(!seqnames)


# function to generate mag rpkm for each sample
ref_mag_bam_rpkm <- function(bam_path){
  
  # identify sample_id from bam file path
    sample_id <- bam_path %>% str_remove(".*/readmap_results/") %>% str_remove("_mapped.bam")

  # get total sequencing read counts for sample from GLAMR
      read_count_fastp <- read_tsv(paste0("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/read_counts/",sample_id,"_read_count_fastp.tsv"))
      read_count_decon_fwd <- read_count_fastp[1, 2]
      read_count_decon_rev <- read_count_fastp[1, 3]
      total_decon_sample_reads <- sum(read_count_decon_fwd, read_count_decon_rev)  # get total read count from GLAMR (all fwd and rev reads)

  # read in bam file
    bam <- Rsamtools::BamFile(file = bam_path,
                            index = paste0(bam_path,".bai"))
    
  # read the BAM file and count aligned reads
    bam_file_df <- as.data.frame(scanBam(bam))
    aligned_read_counts <- as.data.frame(table(bam_file_df$rname))

    mapped_reads_df  <-  aligned_read_counts %>% type_convert() %>% 
                          mutate(mag = str_extract(Var1, pattern)) %>%  # label MAG names
                         group_by(mag) %>% 
                          mutate(mapped_reads = sum(Freq)) %>%  # calculate number of mapped reads per MAG
                     distinct(mag,.keep_all = TRUE) %>%
                          select(-Freq)                         # remove 'Freq' after distinct(), because not relevant for entire MAG
  
    rpkm <- left_join(mapped_reads_df, ref_mag_length, by = "mag") %>%
                 mutate(total_decon_sample_reads,                # add total_decon_sample_reads determined earlier in function
                        sample_id,                               # add sample_id determined earlier in function
                        rpkm = (mapped_reads / (mag_length * total_decon_sample_reads)) * 1000000) %>%  # Calculate RPKM
                  select(-Var1)                                  # remove 'Freq' after distinct(), because not relevant for entire MAG
} 

# create list of bam paths to process
wholebgc_bam_paths_rpkm <- system("ls /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results/*.bam", intern = TRUE) %>%
  tibble(bam_path = .) 

plan(multisession, workers = 24)
options(future.globals.maxSize = 8000 * 1024^2)
wholebgc_kenya_bam_rpkm_mm2 <- future_map_dfr(wholebgc_bam_paths_rpkm$bam_path, ~ ref_mag_bam_rpkm(.x), .progress = TRUE) # last run: Feb 23, 2024
plan(sequential)

head(wholebgc_kenya_bam_rpkm_mm2)

#write and read csv 
#write_csv(wholebgc_kenya_bam_rpkm_mm2, file = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/wholebgc_kenya_bam_rpkm_mm2_Feb24_24.csv") 
#kenya_bam_rpkm_mm2 <- read_csv(file = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/wholebgc_kenya_bam_rpkm_mm2_Feb24_24.csv")


######2. Process toxin marker gene read mapping results to get coverage and identity statistics 

# reference sequence import
ref_seq <- Biostrings::readDNAStringSet("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/extractedbgcs_90_90_rep_seq.fasta") %>%
    as.character() %>%
    data.frame(ref_base = ., seqnames = names(.)) %>%
    mutate(seq_length = nchar(ref_base)) %>%
    separate_longer_position(ref_base, width = 1) %>%
    group_by(seqnames) %>%
    mutate(pos = row_number()) %>%
    ungroup() 

#fix seq names 
ref_seq$seqnames <- sub(" .*", "", ref_seq$seqnames)

# function to generate bam stats - reference cover & % id
bam_stats <- function(bam_path){

  # read in bam file
  bam <- Rsamtools::BamFile(file = bam_path,
                            index = paste0(bam_path,".bai"))
  
  pileup_ref_join <- Rsamtools::pileup(bam,pileupParam = PileupParam(distinguish_strands = FALSE)) %>% 
    full_join(ref_seq, ., by = c("seqnames", "pos"))  # join pileup file and alignment reference

out_bp_depth <- pileup_ref_join %>%
      group_by(seqnames, pos, seq_length) %>% 
    arrange(seqnames, pos, desc(count)) %>% 
      mutate(depth = sum(count),                          # alignment depth at each position
             rel_abund_base = count/depth) %>%            # relative abundance of base read at position compared to all reads at position
    group_by(seqnames) %>% 
      mutate(bam_path = bam_path,
             sample_id = bam_path %>% str_remove(".*bam/") %>% str_remove("_GLAMRsxtAll.*")) %>%
    ungroup()

out_percent_id <- out_bp_depth %>%
      group_by(seqnames) %>%
    filter(nucleotide == ref_base) %>%                    # only look at bases that match reference
      mutate(percent_id_ref_contig = (sum(count) / sum(depth)) * 100) %>%    #all reads matching ref / total reads for sequence            
    ungroup()

out_cover <- out_bp_depth %>%
  filter(count >= 1, na.rm = TRUE) %>%                   # keep only positions with at least 1 count
  distinct(seqnames, pos, .keep_all = TRUE) %>%
    group_by(seqnames) %>%
      mutate(percent_cover = (n()/seq_length) * 100) %>%      # number of bases matching reference divided by reference sequence length 
    ungroup()

out <- left_join(out_percent_id, out_cover)  # merge %id and cover data tables

}

# create list of bam paths to process
bam_paths <- system("ls /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/readmap_results/*.bam", intern = TRUE) %>%
  tibble(bam_path = .) 

# run for bam stats!
plan(multisession, workers = 24)
options(future.globals.maxSize = 8000 * 1024^2)
#whole_bgckenya_bam_stats_mm2 <- map_df(bam_paths$bam_path, ~ bam_stats(.x), .progress = TRUE) #RUN LAST - (date: Feb 24, 2023)
plan(sequential)



#write_csv(whole_bgckenya_bam_stats_mm2, file = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/whole_bgckenya_bam_stats_mm2_feb252024.csv")
#whole_bgckenya_bam_stats_mm2 <- read_csv(file = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_wholebgc_9590/whole_bgckenya_bam_stats_mm2_feb252024.csv", col_names = TRUE)

#Remove rows with NA for 'percent_cover'
whole_bgckenya_bam_stats_mm2 <-whole_bgckenya_bam_stats_mm2 %>% filter(!is.na(percent_cover))

head(whole_bgckenya_bam_stats_mm2)

#Sort for hits having 50% coverage

whole_bgckenya_bam_stats_mm2_80_cov <- whole_bgckenya_bam_stats_mm2 %>%
    group_by(sample_id, seqnames) %>% 
      select(seqnames, sample_id, percent_cover, percent_id_ref_contig) %>% # skim down data table to just data for each ref
      distinct() %>%
    group_by(sample_id) %>%
      mutate(gene_present = percent_cover > 50) 

#Make new df with only true hits
kept_whole_bgckenya_bam_stats_mm2_80_cov = whole_bgckenya_bam_stats_mm2_80_cov %>% filter(gene_present == TRUE)
dim(kept_whole_bgckenya_bam_stats_mm2_80_cov)
# [1] 2541    5



```

10. Visualize read mapping of whole BGCs
```{r}

kept_whole_bgckenya_bam_stats_mm2_80_cov %>% ggplot(aes(x = seqnames, y = percent_cover)) + geom_bar(stat = "identity")

hist(kept_whole_bgckenya_bam_stats_mm2_80_cov$percent_cover)

#Lots of hits at 100 percent coverage so it may even be ok to not use trimmed BGCs? I should go ahead and annotate and run the trimmed BGCs I think since it is more manually curated and go ahead with that data, but good to know that I can use untrimmed BGCs possibly from field data. 
```

#####################################################################Creating Database for Curated BGCs from MAGs from the Field Data

11. Manually curate BGC database and add annotations to them
```{r}
#Manually curated DB is found here: https://docs.google.com/spreadsheets/d/1UaUYG6ll0A2PuafYUBd1-kETdX6J724qV6qq9HPT1ug/edit#gid=217840513

#I ran antismash online so that I could visualize each of these clusters. I then went through and made specific rules for filtering these BGCs for each type of BGC. They are found under What I did on the meta tab. I created new names for each of the BGCs, identified where I want to pull them in the contig (start and end), and made notes for those that I need to revisit for potential of being a more toxic or interesting BGC. I then made this final table into a csv readable file to read in so I can pull the nucleotide sequences for these BGCs. 

#File found here: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/drep95_man_annot_BGCs.csv

#Created a new file to have correct header names for parsing script found here:  /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/parsing_drep95_man_annot.csv
```

12. Pull manually curated BGC database together 
```{r}
#Used this parsing script on this csv: 

#script: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/scripts/extract_bgcs_gbff.py and executed using python extract_bgcs_gbff.py (just need to change input and output)
#csv:  /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/parsing_drep95_man_annot.csv

#Output file of db:  /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/references/drep95_manannot_bgcs_feb262024.fasta
```

#####################################################################Data processing for Figure 9 

13. Read map manually curated BGC database to reads 
```{r}
#Snakefile (running on cayman): /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/Snakefile

# import os
# import re
# import snakemake.io
# from glob import glob
# 
# #--profile /geomicro/data21/lnhart/CSP22_data/config/snakemake_profiles/cayman
# 
# qc_reads = glob_wildcards("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/decon_reads/{sample}_fwd.fastq.gz").sample  
# 
# rule read_mapping_manannot95_kenya:
#     input: 
#         f_reads = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/decon_reads/{sample}_fwd.fastq.gz",
#         r_reads = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/decon_reads/{sample}_rev.fastq.gz",
#         ref = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/references/drep95_manannot_bgcs_feb262024.fasta"
#     output: 
#         sam = temp("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/{sample}_mapped.sam"),
#         temp_bam = temp("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/{sample}_mapped_temp.bam"),
#         unsorted_bam = temp("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/{sample}_mapped_unsorted.bam"),
#         bam = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/{sample}_mapped.bam"
#     conda: "/geomicro/data21/lnhart/Projects/Changing-bloom/usgs-metagenomes/toxin-marker-readmap/minimap2.yaml"
#     log: "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/{sample}_mapped_bam.log"
#     resources: cpus=48
#     shell: 
#         """
#         minimap2 \
#             -ax sr \
#             -t {resources.cpus} \
#             --secondary=no \
#             {input.ref} \
#             {input.f_reads} {input.r_reads} > {output.sam}
# 
#         samtools view -bS {output.sam} > {output.temp_bam}
# 
#         filterBam \
#             --in {output.temp_bam} \
#             --out {output.unsorted_bam} \
#             --minCover 80 \
#             --minId 80
# 
#             samtools sort -o {output.bam} -@ {resources.cpus} {output.unsorted_bam}
#             samtools index -@ {resources.cpus} {output.bam}
#         """
# 
# rule read_mapping_pileup_manannot95_kenya:
#     input:
#         bam = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/{sample}_mapped.bam",
#         ref = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/references/drep95_manannot_bgcs_feb262024.fasta"
#     output:
#         pileup = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/{sample}_pileup.txt"
#     conda: "/geomicro/data21/lnhart/Projects/Changing-bloom/usgs-metagenomes/toxin-marker-readmap/minimap2.yaml"
#     log: "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/{sample}_pileup.log"
#     resources: cpus=48
#     shell:
#         """
#         samtools mpileup -f {input.ref} -o {output.pileup} {input.bam}
#         """
# 
# rule run_read_mapping_manannot95_kenya:
#     input:
#         expand("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/{sample}_pileup.txt", sample = qc_reads)

#Executable code: snakemake run_read_mapping_manannot95_kenya --profile /geomicro/data21/lnhart/CSP22_data/config/snakemake_profiles/cayman --dry-run

#Results: /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results 

#Ran Feb  26, 2024
```

14. Process read mapping
```{r}
######1. Process toxin marker gene read mapping results: RPKM

####Preparing data for function
#% coverage is 80% and 80% identity for this data when run by minimap, can filter for whole BGC coverage in step 2 of this cell

#read in reference sequence fasta 
toxin_MAG_ref <- Biostrings::readDNAStringSet("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/references/drep95_manannot_bgcs_feb262024.fasta") %>%  # import reference fasta
    as.character() %>%
    data.frame(ref_base = ., seqnames = names(.)) %>%
      mutate(mag = str_extract(seqnames, pattern),  # label MAG names
           seq_length = nchar(ref_base))

#Edit sequence names to end where there is a space, not entirely sure there are spaces in these names so let it be until theres an issue 
toxin_MAG_ref$seqnames <- sub(" .*", "", toxin_MAG_ref$seqnames)

#pull sequence names into a vector 
vector = toxin_MAG_ref$seqnames
pattern <- paste(vector, collapse = "|")

ref_mag_length <- toxin_MAG_ref %>%
    group_by(seqnames) %>%
      mutate(mag_length = sum(seq_length)) %>%  # get length of each MAG
    reframe(mag = unique(seqnames),
            mag_length = unique(mag_length))

#remove seqnames from dataframe
ref_mag_length = ref_mag_length %>% select(!seqnames)


# function to generate mag rpkm for each sample
ref_mag_bam_rpkm <- function(bam_path){
  
  # identify sample_id from bam file path
    sample_id <- bam_path %>% str_remove(".*/readmap_results/") %>% str_remove("_mapped.bam")

  # get total sequencing read counts for sample from GLAMR
      read_count_fastp <- read_tsv(paste0("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/read_counts/",sample_id,"_read_count_fastp.tsv"))
      read_count_decon_fwd <- read_count_fastp[1, 2]
      read_count_decon_rev <- read_count_fastp[1, 3]
      total_decon_sample_reads <- sum(read_count_decon_fwd, read_count_decon_rev)  # get total read count from GLAMR (all fwd and rev reads)

  # read in bam file
    bam <- Rsamtools::BamFile(file = bam_path,
                            index = paste0(bam_path,".bai"))
    
  # read the BAM file and count aligned reads
    bam_file_df <- as.data.frame(scanBam(bam))
    aligned_read_counts <- as.data.frame(table(bam_file_df$rname))

    mapped_reads_df  <-  aligned_read_counts %>% type_convert() %>% 
                          mutate(mag = str_extract(Var1, pattern)) %>%  # label MAG names
                         group_by(mag) %>% 
                          mutate(mapped_reads = sum(Freq)) %>%  # calculate number of mapped reads per MAG
                     distinct(mag,.keep_all = TRUE) %>%
                          select(-Freq)                         # remove 'Freq' after distinct(), because not relevant for entire MAG
  
    rpkm <- left_join(mapped_reads_df, ref_mag_length, by = "mag") %>%
                 mutate(total_decon_sample_reads,                # add total_decon_sample_reads determined earlier in function
                        sample_id,                               # add sample_id determined earlier in function
                        rpkm = (mapped_reads / (mag_length * total_decon_sample_reads)) * 1000000) %>%  # Calculate RPKM
                  select(-Var1)                                  # remove 'Freq' after distinct(), because not relevant for entire MAG
} 

# create list of bam paths to process
curatedbgc_bam_paths_rpkm <- system("ls /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/*.bam", intern = TRUE) %>%
  tibble(bam_path = .) 

plan(multisession, workers = 36)
options(future.globals.maxSize = 8000 * 1024^2)
curatedbgc_kenya_bam_rpkm_mm2 <- future_map_dfr(curatedbgc_bam_paths_rpkm$bam_path, ~ ref_mag_bam_rpkm(.x), .progress = TRUE) # last run: March 4, 2024
plan(sequential)

head(curatedbgc_kenya_bam_rpkm_mm2)

#write and read csv 
#write_csv(curatedbgc_kenya_bam_rpkm_mm2, file = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/curatedbgc_kenya_bam_rpkm_mm2_March5_24.csv") 

#curatedbgc_kenya_bam_rpkm_mm2 <- read_csv(file = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/curatedbgc_kenya_bam_rpkm_mm2_March5_24.csv")


######2. Process toxin marker gene read mapping results to get coverage and identity statistics 

# reference sequence import
ref_seq <- Biostrings::readDNAStringSet("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/references/drep95_manannot_bgcs_feb262024.fasta") %>%
    as.character() %>%
    data.frame(ref_base = ., seqnames = names(.)) %>%
    mutate(seq_length = nchar(ref_base)) %>%
    separate_longer_position(ref_base, width = 1) %>%
    group_by(seqnames) %>%
    mutate(pos = row_number()) %>%
    ungroup() 

#fix seq names 
ref_seq$seqnames <- sub(" .*", "", ref_seq$seqnames)

# function to generate bam stats - reference cover & % id
bam_stats <- function(bam_path){

  # read in bam file
  bam <- Rsamtools::BamFile(file = bam_path,
                            index = paste0(bam_path,".bai"))
  
  pileup_ref_join <- Rsamtools::pileup(bam,pileupParam = PileupParam(distinguish_strands = FALSE)) %>% 
    full_join(ref_seq, ., by = c("seqnames", "pos"))  # join pileup file and alignment reference

out_bp_depth <- pileup_ref_join %>%
      group_by(seqnames, pos, seq_length) %>% 
    arrange(seqnames, pos, desc(count)) %>% 
      mutate(depth = sum(count),                          # alignment depth at each position
             rel_abund_base = count/depth) %>%            # relative abundance of base read at position compared to all reads at position
    group_by(seqnames) %>% 
      mutate(bam_path = bam_path,
             sample_id = bam_path %>% str_remove(".*bam/") %>% str_remove("_GLAMRsxtAll.*")) %>%
    ungroup()

out_percent_id <- out_bp_depth %>%
      group_by(seqnames) %>%
    filter(nucleotide == ref_base) %>%                    # only look at bases that match reference
      mutate(percent_id_ref_contig = (sum(count) / sum(depth)) * 100) %>%    #all reads matching ref / total reads for sequence            
    ungroup()

out_cover <- out_bp_depth %>%
  filter(count >= 1, na.rm = TRUE) %>%                   # keep only positions with at least 1 count
  distinct(seqnames, pos, .keep_all = TRUE) %>%
    group_by(seqnames) %>%
      mutate(percent_cover = (n()/seq_length) * 100) %>%      # number of bases matching reference divided by reference sequence length 
    ungroup()

out <- left_join(out_percent_id, out_cover)  # merge %id and cover data tables

}

# create list of bam paths to process
bam_paths <- system("ls /geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/readmap_results/*.bam", intern = TRUE) %>%
  tibble(bam_path = .) 

# run for bam stats!
plan(multisession, workers = 36)
options(future.globals.maxSize = 8000 * 1024^2)
curate_bgckenya_bam_stats_mm2 <- map_df(bam_paths$bam_path, ~ bam_stats(.x), .progress = TRUE) #RUN LAST - (date: March 5, 2023)
plan(sequential)

#Remove rows with NA for 'percent_cover'
curate_bgckenya_bam_stats_mm2 <-curate_bgckenya_bam_stats_mm2 %>% filter(!is.na(percent_cover))

#write_csv(curate_bgckenya_bam_stats_mm2, file = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/curate_bgckenya_bam_stats_mm2_march52024.csv")

#curate_bgckenya_bam_stats_mm2 <- read_csv(file = "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/curate_bgckenya_bam_stats_mm2_march52024.csv", col_names = TRUE)

head(curate_bgckenya_bam_stats_mm2)

#Sort for hits having 70% coverage

curate_bgckenya_bam_stats_mm2_70_cov <- curate_bgckenya_bam_stats_mm2 %>%
    group_by(sample_id, seqnames) %>% 
      select(seqnames, sample_id, percent_cover, percent_id_ref_contig) %>% # skim down data table to just data for each ref
      distinct() %>%
    group_by(sample_id) %>%
      mutate(gene_present = percent_cover > 70) 

#Make new df with only true hits
kept_curate_bgckenya_bam_stats_mm2_70_cov = curate_bgckenya_bam_stats_mm2_70_cov %>% filter(gene_present == TRUE)
dim(kept_curate_bgckenya_bam_stats_mm2_70_cov)

#Separate sample ID out 
kept_curate_bgckenya_bam_stats_mm2_70_cov_edit = kept_curate_bgckenya_bam_stats_mm2_70_cov %>% mutate(sample_id = gsub(".*(samp_\\d+).*", "\\1", sample_id))

```

15. Merge data and prepare data to be plotted from read mapping (merge bam and stats file with metadata)
```{r}
#PREPARING DATA TO BE PLOTTED

#Read in db key
man_annot95 = read.csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/drep_kenyamanannot_95_key.csv")

#Adjust db key to match mapping data
man_annot95$new_header <- sub(" .*", "", man_annot95$new_header)
names(man_annot95)[names(man_annot95) == "new_header"] <- "seqnames"

#Merge db_key with stats for mapping 
manannot_bgc_70_stats_key = left_join(kept_curate_bgckenya_bam_stats_mm2_70_cov_edit, man_annot95, by = "seqnames")
dim(manannot_bgc_70_stats_key)
#[1] 4471   12

#Merge RPKM with stats 
names(curatedbgc_kenya_bam_rpkm_mm2)[names(curatedbgc_kenya_bam_rpkm_mm2) == "mag"] <- "seqnames"

manannot_bgc_70_stats_rpkm_key = left_join(manannot_bgc_70_stats_key, curatedbgc_kenya_bam_rpkm_mm2, by = c("sample_id", "seqnames"))
dim(manannot_bgc_70_stats_rpkm_key)
#[1] 4471   16



#Write csv
#write.csv(manannot_bgc_70_stats_rpkm_key, "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/manannot_bgc_70_stats_rpkm_key_mar52024.csv")

#Read csv
#manannot_bgc_70_stats_rpkm_key = read.csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/data/bgc_mining/data/quality_bins/drep_99/antismash_bins99/antismash_drep_BGCs/readmap_manannot_drep95_bgc/manannot_bgc_70_stats_rpkm_key_mar52024.csv")

#Fix issue from file with name
manannot_bgc_70_stats_rpkm_key$grouping[manannot_bgc_70_stats_rpkm_key$grouping == "#VALUE!"] <- "RiPP-like-Proteusin"


#################1. Add in metadata

sample_table = read.csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/metadata/GLOD_LH_Mar52024.csv") 

Kenya_sample_table <- sample_table %>% filter(StudyID == "set_55" | StudyID == "set_57") #include only StudyID's of interest (Kenya)

Kenya_sample_table %>% distinct(SampleID) %>% count #51 WLE metagenome samples in Kenya dataset - Jan 20, 2024

#Select only columns needed--columns needed added March 6, 2024  
Kenya_meta = Kenya_sample_table %>% select(c(site_name_sitenumber, SampleID, SampleName, StudyID, geo_loc_name, lat, lon, collection_date, depth, depth_at_sampling_location, pH, temp, nitrate, diss_oxygen, conduc, secchi, turbidity, part_microcyst, chlorophyl, diss_phosp, ammonia, Nitrate_Nitrite, urea, silicate, sky, Notes, total_sonde, cyano_sonde, replicate_id, orp, particulate_cyl, atmospheric_temp, salinity, ammended_site_name, env_Kenya_site, tot_nit, soluble_react_phosp, tot_phos, site_number, station_description, type, LH_classifier))

#Manipulate date column
Kenya_meta = Kenya_meta %>% mutate(collection_date = mdy(collection_date),
         year = year(collection_date),
         month = month(collection_date),
         day = day(collection_date))

Kenya_meta = Kenya_meta %>% mutate(yday = yday(collection_date), week = week(collection_date))

#Align sample ID column to be able to merge 

colnames(manannot_bgc_70_stats_rpkm_key)[colnames(manannot_bgc_70_stats_rpkm_key) == "sample_id"] <- "SampleID"

bgc70_meta = left_join(manannot_bgc_70_stats_rpkm_key, Kenya_meta, by = "SampleID")

#2. Remove samples not in final analysis 
bgc70_meta = bgc70_meta %>% filter(!SampleID == "samp_4327", !SampleID == "samp_4328", !SampleID == "samp_4329", !SampleID == "samp_4330", !SampleID == "samp_4455", !SampleID == "samp_4456", !SampleID == "samp_4453", !SampleID == "samp_4315", !SampleID == "samp_4316")

bgc70_meta = bgc70_meta %>% filter(!SampleID == "samp_4325", !SampleID == "samp_4326")

##############3. Normalize for duplicates in 2022 samples

bgc70_meta22 = bgc70_meta %>% filter(year == 2022)  #1778
bgc70_meta22 = bgc70_meta22 %>% group_by(site_name_sitenumber, seqnames) %>% mutate(summed_rpkm = mean(rpkm), total_bgc_covered = mean(percent_cover))
dim(bgc70_meta22)
#[1] 1778   64

bgc70_meta22 = unique(bgc70_meta22[c('site_name_sitenumber', 'seqnames', 'summed_rpkm', 'total_bgc_covered')])
dim(bgc70_meta22)
#[1] 938   4

bgc70_meta22$year <- 2022

#get 2023 data 
bgc70_meta23 = bgc70_meta %>% ungroup() %>% filter(year == 2023) %>% select(site_name_sitenumber, seqnames, rpkm, percent_cover, year)
bgc70_meta23 = bgc70_meta23 %>% mutate(summed_rpkm = rpkm) %>% select(!rpkm) %>% mutate(total_bgc_covered = percent_cover) %>% select(!percent_cover)
dim(bgc70_meta23)
#[1] 2132    5

#Combine 22 and 23 data together 
bgc70_metanorm = rbind(bgc70_meta22, bgc70_meta23)
dim(bgc70_metanorm)
#[1] 3070    5

#Merge in database key
bgc70_metanorm = left_join(bgc70_metanorm, man_annot95, by = "seqnames")

```

#####################################################################Making Figure 8 and 9

16. Make pub ready figure of BGC mapping for holistic biosynthetic potential (Figure 8)
```{r}
#Combine bgc70_metanorm with genus data on bins 
#Load postgres server running on Alpena
pg <- DBI::dbConnect(RPostgres::Postgres(),dbname = "glamr_data", host = "localhost", port = "5432", user = "glamr_admin", password = "glamr2023")

#Load GLAMR GTDBTK data
# gtdb <- tbl(pg, "GTDB") %>% 
#   collect() %>% 
#   mutate(sample = str_extract(user_genome, "samp_\\d+")) %>% 
#   relocate(user_genome, sample) 

# #Load GLAMR CheckM data
# checkM <- tbl(pg, "checkm") %>% 
#   collect() %>% 
#   mutate(sample = str_extract(`Bin Id`, "samp_\\d+")) %>% 
#   relocate(`Bin Id`, sample)

#gtdb table loaded in first chunk
GLAMR_GTDB_samp_oi_bgc <- gtdb %>% filter(user_genome %in% bgc70_metanorm$file)
GLAMR_GTDB_samp_oi_bgc = GLAMR_GTDB_samp_oi_bgc %>% select(c(user_genome, classification))

bgc70_metanorm_bgc = left_join(bgc70_metanorm, GLAMR_GTDB_samp_oi_bgc, by = c("file" = "user_genome"))

#Fix classification to be separated 
bgc70_metanorm_bgc = bgc70_metanorm_bgc %>% separate(classification, into = c("Domain", "Phylum", "Class", "Order", "Family", "Genus", "Species"), sep = ";[a-z]__") %>%  mutate(Domain = str_remove(Domain, "d__"))

#Select columns for file
bgc70_bgcsonly_metanorm_bgc = bgc70_metanorm_bgc %>% ungroup()  %>% select(c(file, pathway, manual_identifier, grouping, Domain, Phylum, Class, Order, Family, Genus, Species)) %>% distinct()

#Fix genus category to match other genera names used before from file 1
bgc70_bgcsonly_metanorm_bgc = bgc70_bgcsonly_metanorm_bgc %>% mutate(
    Genus = if_else(Genus == "NIES-981", "Cyanobium", Genus),
    Genus = if_else(Genus == "LMEP-6097", "Caenarcaniphilales", Genus),
    Genus = if_else(Genus == "UBA5018", "Unclassified", Genus),
    Genus = if_else(Genus == "CAIQIA01", "Unclassified", Genus),
    Genus = if_else(Genus == "CAIXOZ01", "Unclassified", Genus),
    Genus = if_else(Genus == "CBW1002", "Synechococcus", Genus),
    Genus = if_else(is.na(Genus) | Genus == "", "Unclassified", Genus),
  )

#Make count file for each bin 
bgc70_bgcsonly_metanorm_bgc_count <- bgc70_bgcsonly_metanorm_bgc %>%
  group_by(file) %>%
  mutate(bgc_perbin = n_distinct(manual_identifier))

bgc70_bgcsonly_metanorm_bgc_count = bgc70_bgcsonly_metanorm_bgc_count %>% select(c(file, Genus, bgc_perbin)) %>% distinct() %>% filter(!Genus == "Caenarcaniphilales")

n <- 180
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))

#col_vector[1] <- "#8B4513"
col_vector[1] <- "deeppink"
col_vector[2] <- "salmon"
col_vector[3] <- "#00A000"
col_vector[4] <- "yellow3"
col_vector[5] <- "#009099"
col_vector[6] <- "orange"
col_vector[7] <- "#9A8DA6"
col_vector[8] <- "lightblue"
col_vector[9] <- "red"
col_vector[10] <- "lightpink"
col_vector[11] <- "#CC5500"
col_vector[12] <- "#FFDAB9"
col_vector[13] <- "gray3"
col_vector[14] <- "#8B8000"


# bgc70_bgcsonly_metanorm_bgc_count %>% ggplot(aes(x = Genus, y = bgc_perbin, fill = Genus)) + geom_boxplot() + geom_jitter(width = 0.1, alpha = 0.5) + theme_bw() +scale_x_discrete(guide = guide_axis(angle = 45)) + scale_fill_manual(values = col_vector)

bgc70_bgcsonly_metanorm_bgc_count %>%
  ggplot(aes(x = reorder(Genus, bgc_perbin), y = bgc_perbin, fill = Genus)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_fill_manual(values = col_vector) + guides(fill = FALSE) + labs(x = 'Genus', y = 'BGCs per MAG') + theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, face = "bold", size = 11), axis.text.y = element_text(face = "bold",size = 12), axis.title.y = element_text(face = "bold", size = 12), axis.title.x = element_text(face = "bold", size = 12))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperbin.png", width = 4, height =4, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperbin.tiff", width = 4, height =4, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperbin.svg", width = 4, height =4, dpi=300, scale =2, limitsize = FALSE))

#####Visualize # of bgcs in each biosynthetic class 
bgcs_pathways = read.csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/cytoscape_nodes_table.csv")
bgcs_pathways2 = bgcs_pathways %>% select(c(BiG.SCAPE.class, Genus))
#remove unneeded genera 
bgcs_pathways2 = bgcs_pathways2 %>% filter(!Genus == "Unclassified Vampirovibrionales", !Genus == "Phormidium", !Genus == "Nostoc", !Genus == "Fischerella", !Genus == "Anabaena")

bgcs_pathways2 = bgcs_pathways2 %>% mutate(BiG.SCAPE.class = ifelse(BiG.SCAPE.class == "PKS-NRP_Hybrids", "PKS-NRPS", BiG.SCAPE.class)) %>% mutate(BiG.SCAPE.class = ifelse(BiG.SCAPE.class == "PKSI", "T1PKS", BiG.SCAPE.class)) %>% mutate(BiG.SCAPE.class = ifelse(BiG.SCAPE.class == "PKSother", "Other PKS", BiG.SCAPE.class))

	
n <- 180
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))

col_vector[1] <- "deeppink"
col_vector[2] <- "salmon"
col_vector[3] <- "#00A000"
col_vector[4] <- "yellow3"
col_vector[5] <- "#009099"
col_vector[6] <- "orange"
col_vector[7] <- "#9A8DA6"
col_vector[8] <- "lightblue"
col_vector[9] <- "red"
col_vector[10] <- "lightpink"
col_vector[11] <- "#CC5500"
col_vector[12] <- "#FFDAB9"
col_vector[13] <- "gray3"
col_vector[14] <- "#8B8000"

bgcs_pathways2 %>%
  group_by(Genus, BiG.SCAPE.class) %>%
  summarise(count = n()) %>% 
  ggplot(aes(y = Genus, x = BiG.SCAPE.class, fill = Genus, size = count)) +
  geom_point(shape = 21) +
  scale_fill_manual(values = col_vector) +
  scale_size_continuous(range = c(3, 8), breaks = c(5, 10, 15, 20)) +  # Specify breaks for size legend
  labs(y = "BiG-SCAPE Class", x = "Genus", fill = "Genus", size = "Number of BGCs") +
  theme_bw() + 
  guides(fill = FALSE) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, face = "bold"), 
        axis.text.y = element_text(face = "bold"), 
        axis.title.y = element_text(face = "bold", size = 12), 
        axis.title.x = element_text(face = "bold", size = 12))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperpathway.png", width = 4, height =3, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperpathway.tiff", width = 4, height =3, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperpathway.svg", width = 4, height =3, dpi=300, scale =2, limitsize = FALSE))

```


16. (continued) MAKING FIGURES 8 WITH NON DEREPLICATED SET OF BGCS TO GET FULL COUNTS
```{r}
#Read in multismash results, which have been filtered to not include any BGCs smaller than 5,000 nucleotides 
multismash_parse = read.csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/multimash_nodrep_parsingfile.csv")

#Merge genus annotations to this 
genomes = read_csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/genomes_genera.csv")
quality = read_csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/checkm_gtdb_results_22-23-Dec-22-2023.csv")
quality2 = quality %>% select(c(genome, Domain, Phylum, Class, Order, Genus, Species))
colnames(multismash_parse)[colnames(multismash_parse) == "file"] <- "genome"

full_multismash = left_join(multismash_parse, quality2, by = "genome")

#Fix genus names 
full_multismash2 = full_multismash %>% mutate(
    Genus = if_else(Genus == "NIES-981", "Cyanobium", Genus),
    Genus = if_else(Genus == "LMEP-6097", "Caenarcaniphilales", Genus),
    Genus = if_else(Genus == "UBA5018", "Unclassified", Genus),
    Genus = if_else(Genus == "CAIQIA01", "Unclassified", Genus),
    Genus = if_else(Genus == "CAIXOZ01", "Unclassified", Genus),
    Genus = if_else(Genus == "CBW1002", "Synechococcus", Genus),
    Genus = if_else(is.na(Genus) | Genus == "", "Unclassified", Genus),
  )

#Plot average bgcs in a bin per genera

full_multismash2_count <- full_multismash2 %>%
  group_by(genome) %>%
  mutate(bgc_perbin = n_distinct(product))

full_multismash2_count = full_multismash2_count %>% select(c(genome, Genus, bgc_perbin)) %>% distinct() %>% filter(!Genus == "Caenarcaniphilales", !Genus == "CAIUCS01")

n <- 180
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))

#col_vector[1] <- "#8B4513"
col_vector[1] <- "deeppink"
col_vector[2] <- "salmon"
col_vector[3] <- "#00A000"
col_vector[4] <- "yellow3"
col_vector[5] <- "#009099"
col_vector[6] <- "orange"
col_vector[7] <- "#9A8DA6"
col_vector[8] <- "lightblue"
col_vector[9] <- "red"
col_vector[10] <- "lightpink"
col_vector[11] <- "#CC5500"
col_vector[12] <- "#FFDAB9"
col_vector[13] <- "gray3"
col_vector[14] <- "#8B8000"

full_multismash2_count %>%
  ggplot(aes(x = reorder(Genus, bgc_perbin), y = bgc_perbin, fill = Genus)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_fill_manual(values = col_vector) + guides(fill = FALSE) + labs(x = 'Genus', y = 'BGCs per MAG') + theme_cowplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, face = "bold", size = 11), axis.text.y = element_text(face = "bold",size = 12), axis.title.y = element_text(face = "bold", size = 12), axis.title.x = element_text(face = "bold", size = 12))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperbin_nondrep_stack.png", width = 4, height =4, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperbin_nondrep_stack.tiff", width = 4, height =4, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperbin_nondrep_stack.svg", width = 4, height =4, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcperbin_nondrep_stack.eps", width = 4, height =4, dpi=300, scale =2, limitsize = FALSE))

#####Visualize # of bgcs in each biosynthetic class 

full_multismash2_path = full_multismash2 %>% filter(!Genus == "Caenarcaniphilales", !Genus == "CAIUCS01")

#Too many biosynthetic products so needed to make them into classes, first need to get a distinct list of the products to copy into excel
products = full_multismash2_path %>% select(product) %>% distinct()
write.csv(products, "/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/products.csv")
products_key = read.csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/products_key.csv")

#Merge files 
full_multismash2_path_key = left_join(full_multismash2_path, products_key, by = "product")

#Reorder to be in the same order as the previous figure 
#Synechococcus, Vulcanococcus, Unclassified, Cyanobium, Pseudanabaena, Rhapidiopsis, Nodosilinea, Snowella, Elainella, Planktothrix, Cuspidothrix, Dolichospermum, Microcystis, Sphaerospermopsis

########Fix coloring!
n <- 180
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))

col_vector[11] <- "deeppink"
col_vector[4] <- "salmon"
col_vector[12] <- "#00A000"
col_vector[9] <- "yellow3"
col_vector[13] <- "#009099"
col_vector[7] <- "orange"
col_vector[10] <- "#9A8DA6"
col_vector[5] <- "lightblue"
col_vector[6] <- "red"
col_vector[8] <- "lightpink"
col_vector[14] <- "#CC5500"
col_vector[1] <- "#FFDAB9"
col_vector[3] <- "gray3"
col_vector[2] <- "#8B8000"

# Define the custom order for Genus
custom_order <- c("Synechococcus", "Vulcanococcus", "Unclassified", "Cyanobium", "Pseudanabaena", "Raphidiopsis", "Nodosilinea", "Snowella", "Elainella", "Planktothrix", "Cuspidothrix", "Dolichospermum", "Microcystis", "Sphaerospermopsis")

#Cyanobium heterocyst glycolipid classifications moved to Other because likely not those 


# Reorder Genus as a factor with custom order
full_multismash2_path_key %>%
  group_by(Genus, product_class) %>%
  summarise(count = n()) %>% 
  mutate(Genus = factor(Genus, levels = custom_order)) %>%
  ggplot(aes(x = Genus, y = product_class, fill = Genus, size = count)) +
  geom_point(shape = 21) +
  scale_fill_manual(values = col_vector) +
  scale_size_continuous(range = c(2, 8), breaks = c(25, 50, 75, 100, 125, 150)) +  # Specify breaks for size legend
  labs(y = "Product Class", x = "Genus", fill = "Genus", size = "Number of BGCs") +
  theme_bw() + 
  guides(fill = FALSE) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, face = "bold"), 
        axis.text.y = element_text(face = "bold"), 
        axis.title.y = element_text(face = "bold", size = 12), 
        axis.title.x = element_text(face = "bold", size = 12))


full_multismash2_path_key %>%
  # Adjusting the product_class conditionally
  mutate(product_class = if_else(Genus == "Cyanobium" & product_class == "Heterocyst Glycolipid", "Other", product_class)) %>%
  # Grouping by Genus and product_class
  group_by(Genus, product_class) %>%
  # Summarising to count occurrences
  summarise(count = n(), .groups = 'drop') %>%
  # Factorizing Genus with custom order (make sure custom_order is defined)
  mutate(Genus = factor(Genus, levels = custom_order)) %>%
  # Plotting
  ggplot(aes(x = Genus, y = product_class, fill = Genus, size = count)) +
    geom_point(shape = 21) +
    scale_fill_manual(values = col_vector) +  # Ensure col_vector is defined
    scale_size_continuous(range = c(2, 8), breaks = c(25, 50, 75, 100, 125, 150)) +  # Adjusted size legend
    labs(y = "Product Class", x = "Genus", fill = "Genus", size = "Number of BGCs") +
    theme_bw() + 
    guides(fill = FALSE) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, face = "bold"), 
          axis.text.y = element_text(face = "bold"), 
          axis.title.y = element_text(face = "bold", size = 12), 
          axis.title.x = element_text(face = "bold", size = 12))


(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgctypes_nodrep.png", width = 4, height =2, dpi=300, scale =2, limitsize = FALSE))
(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgctypes_nodrep.svg", width = 4, height =2, dpi=300, scale =2, limitsize = FALSE))
(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgctypes_nodrep.tiff", width = 4, height =2, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgctypes_nodrep.eps", width = 4, height =2, dpi=300, scale =2, limitsize = FALSE))

```

17. Making bubble plot figure 9
```{r}
#######Make dataframe for plotting 
bgc_data <- bgc70_metanorm %>% ungroup() %>% select(c(grouping, site_name_sitenumber, summed_rpkm))
bgc_data_summed <- bgc_data %>%
  group_by(site_name_sitenumber,grouping) %>%
  summarise(summed_rpkm = sum(summed_rpkm, na.rm = TRUE)) %>%
  pivot_wider(names_from = grouping, values_from = summed_rpkm)

bgc_data_summed <- replace(bgc_data_summed, is.na(bgc_data_summed), 0)
bgc_data_summed2 <- bgc_data_summed[, -1]
rownames(bgc_data_summed2) <- bgc_data_summed$site_name_sitenumber


###############Make bubble plot at grouping level for stations + Hierarchically cluster stations + BGCs at bottom
#fix spliceotide spelling

bgc_data$grouping <- gsub("Splideotide", "Spliceotide", bgc_data$grouping)

bgc_data_sum = bgc_data %>% group_by(site_name_sitenumber,grouping) %>%
  summarise(summed_rpkm = sum(summed_rpkm))

#Order dataset based on site
order_vector <- c(
  "1. Homa Bay", "2. Homa Bay Pier", "3. Homa Bay Water Intake", "4. Soklo", "5. Mbita East",
  "6. Mbita West", "7. Asembo Bay", "8. Ndere Island", "9. Mid Gulf", "10. Kisumu Harbor",
  "11. Dunga","12. Nyando River Mouth","13. Sondu River Bridge", "14. Sondu River Shore","15. Sondu Miriu","16. Kibuon River", "17. Southern Mid-Gulf", "18. Awach River Mouth",
  "19. Bala Rawi", "20. Ingra", "21. Kowuor", "22. Oluch", "23. Sikli", "24. Mirunda",
  "25. Mbita East", "26. Mbita West", "27. Bridge Island", "28. Bondo", "29. Yala River Mouth",
  "30. Rosinga Channel", "31. South Rosinga"
)

#meta_bam_summary_complete_all_70$site_name_sitenumber <- factor(meta_bam_summary_complete_all_70$site_name_sitenumber, levels = order_vector)

order_vector_reverse_70 <- rev(order_vector)

bgc_data_sum$site_name_sitenumber <- factor(bgc_data_sum$site_name_sitenumber, levels = order_vector_reverse)

unique(bgc_data_sum$site_name_sitenumber)

#ordered
bgc_data_sum %>% filter(!grouping == "#VALUE!") %>% 
  ggplot(aes(y = site_name_sitenumber, x = grouping, size = summed_rpkm, fill = grouping)) +
  geom_point(shape = 21) +
  scale_fill_manual(values = col_vector) +
  scale_size_continuous(range = c(1, 10)) +
  labs(y = "Sampling Site", x = "Biosynthetic Gene Cluster (BGC)", fill = "Genus of BGC", size = "Total RPKM") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, face = "bold"), axis.text.y = element_text(face = "bold"), axis.title.y = element_text(face = "bold", size = 12), axis.title.x = element_text(face = "bold", size = 12)) + guides(fill = guide_legend(override.aes = list(size = 5)))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcrpkm_bubble.png", width = 10, height =4, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcrpkm_bubble.tiff", width = 10, height =4, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcrpkm_bubble.svg", width = 10, height =4, dpi=300, scale =2, limitsize = FALSE))

#Hierarchically cluster stations and display dendogram of those and add 4 missing river stations 

bgc_data_reshaped <- bgc_data_sum %>%
  group_by(site_name_sitenumber,grouping) %>%
  pivot_wider(names_from = grouping, values_from = summed_rpkm)

bgc_data_reshaped <- replace(bgc_data_reshaped, is.na(bgc_data_reshaped), 0)
bgc_data_reshaped2 <- bgc_data_reshaped[, -1]
rownames(bgc_data_reshaped2) <- bgc_data_reshaped$site_name_sitenumber

new_rows <- matrix(0, nrow = 4, ncol = ncol(bgc_data_reshaped2), dimnames = list(c("12. Nyando River", "13. Sondu River Bride", "14. Sondu River Shore", "16. Kibuon River"), colnames(bgc_data_reshaped2)))
# Bind the new rows to the existing matrix
bgc_data_reshaped2 <- rbind(bgc_data_reshaped2, new_rows)

dist_matrix <- dist(bgc_data_reshaped2, method = "euclidean")

hc <- hclust(dist_matrix)

tiff("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_dendro.tiff")
svg("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_dendro.svg")
png("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_dendro.png")
plot(hc, hang = -1, cex = 0.6)
dev.off()

#Add rivers to bgc data sum
new_stations <- data.frame(site_name_sitenumber = c("12. Nyando River", "13. Sondu River Bridge", "14. Sondu River Shore", "16. Kibuon River"),
                            grouping = "NRPS-like",
                            summed_rpkm = NA)


# Combine new_stations with bgc_data_sum
bgc_data_sum_updated <- bind_rows(bgc_data_sum, new_stations)


order_vector_bgc <- c(
  "28. Bondo", "29. Yala River Mouth", "3. Homa Bay Water Intake", "2. Homa Bay Pier", "4. Soklo",
  "11. Dunga", "20. Ingra", "19. Bala Rawi", "16. Kibuon River", "14. Sondu River Shore",
  "12. Nyando River","13. Sondu River Bridge","31. South Rosinga", "18. Awach River Mouth","22. Oluch","9. Mid Gulf", "24. Mirunda", "7. Asembo Bay",
  "15. Sondu Miriu", "10. Kisumu Harbor", "17. Southern Mid-Gulf", "8. Ndere Island", "21. Kowuor", "25. Mbita East",
  "5. Mbita East", "26. Mbita West", "30. Rosinga Channel", "6. Mbita West", "27. Bridge Island",
  "1. Homa Bay", "23. Sikli"
)

#meta_bam_summary_complete_all_70$site_name_sitenumber <- factor(meta_bam_summary_complete_all_70$site_name_sitenumber, levels = order_vector)

order_vector_reverse_70 <- rev(order_vector_bgc)

bgc_data_sum_updated$site_name_sitenumber <- factor(bgc_data_sum_updated$site_name_sitenumber, levels = order_vector_reverse_70)

unique(bgc_data_sum_updated$site_name_sitenumber)

unique(bgc_data_sum_updated$grouping)

#read in groupings info
groupings = read.csv("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL2/tables/groupings.csv")

#merge groupings
bgc_data_sum_updated2 = left_join(bgc_data_sum_updated, groupings, by = "grouping")

#########MAKE FINAL PLOT

#colors
n <- 180
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))

col_vector[1] <- "blue"
col_vector[2] <- "skyblue"
col_vector[3] <- "pink"
col_vector[4] <- "green"
col_vector[5] <- "darkgreen"
col_vector[6] <- "magenta"
col_vector[7] <- "red"
col_vector[8] <- "deepskyblue"
col_vector[9] <- "chartreuse3"
col_vector[10] <- "gold"
col_vector[11] <- "goldenrod3"
col_vector[12] <- "yellow1"
col_vector[13] <- "purple"
col_vector[14] <- "mediumpurple2"
col_vector[15] <- "darkmagenta"
col_vector[16] <- "forestgreen"
col_vector[17] <- "orange"
col_vector[18] <- "darkorange3"
col_vector[19] <- "brown"
col_vector[20] <- "darkolivegreen1"
col_vector[21] <- "darkolivegreen4"
col_vector[22] <- "deepskyblue4"
col_vector[23] <- "darkturquoise"
col_vector[24] <- "darkslategrey"
col_vector[25] <- "coral"
col_vector[26] <- "maroon"
col_vector[27] <- "tan1"
col_vector[28] <- "lightpink2"
col_vector[29] <- "tan3"
col_vector[30] <- "springgreen"
col_vector[31] <- "indianred"
col_vector[32] <- "saddlebrown"
col_vector[33] <- "steelblue3"
col_vector[34] <- "lavenderblush3"
col_vector[35] <- "indianred1"
col_vector[36] <- "orangered1"
col_vector[37] <- "tomato3"
col_vector[38] <- "lavenderblush4"
col_vector[39] <- "peachpuff3"
col_vector[40] <- "red4"
col_vector[41] <- "firebrick"
col_vector[42] <- "yellowgreen"

# Plot ggplot visualization--FINAL BUBBLE PLOT VISUALIZATION
bgc_data_sum_updated2 %>% filter(!grouping == "#VALUE!") %>% 
  ggplot(aes(y = site_name_sitenumber, x = grouping, size = summed_rpkm, fill = grouping)) +
  geom_point(shape = 21) +
  scale_fill_manual(values = col_vector) +
  scale_size_continuous(range = c(2, 10)) +
  labs(y = "Sampling Site", x = "Biosynthetic Gene Cluster (BGC) Type", size = "Total RPKM") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, face = "bold"), axis.text.y = element_text(face = "bold"), axis.title.y = element_text(face = "bold", size = 12), axis.title.x = element_text(face = "bold", size = 12), strip.text = element_text(face = "bold", color = "black"), strip.background = element_rect(fill = "white", color = "gray")) + guides(fill = guide_legend(override.aes = list(size = 5))) + facet_grid(~high_level_grouping, scales = "free_x")

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_rpkm_unknownbgcs.png", width = 14, height =4, dpi=300, scale =2, limitsize = FALSE))
(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_rpkm_unknownbgcs.tiff", width = 14, height =4, dpi=300, scale =2, limitsize = FALSE))
(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_rpkm_unknownbgcs.svg", width = 14, height =4, dpi=300, scale =2, limitsize = FALSE))


```


18. Making SI Figure 4 (NMDSs plotting of BGCs in relation to dominant bloom-formers relative abundance patterns) 
```{r}
######Using all 70% covered bgcs, lowest % id is 91%

############PREPARE DATAFRAMES #########
bgc_data <- bgc70_metanorm %>% ungroup() %>% select(c(manual_identifier, site_name_sitenumber, summed_rpkm))
bgc_data = bgc_data %>% pivot_wider(names_from = manual_identifier, values_from = summed_rpkm)
bgc_data <- replace(bgc_data, is.na(bgc_data), 0)
bgc_data2 <- bgc_data[, -1]
rownames(bgc_data2) <- bgc_data$site_name_sitenumber
dim(bgc_data2)
#[1]  27 267 ##should be 31 so need to add the 4 missing sites to the matrix and give 0s for those: 12. Nyando River, 13. Sondu River Bride, 14. Sondu River Shore, 16. Kibuon River ---- DONT NEED THESE BC EMPTY ROWS
# new_rows <- matrix(0, nrow = 4, ncol = ncol(bgc_data2),
#                    dimnames = list(c("12. Nyando River", "13. Sondu River Bride", "14. Sondu River Shore", "16. Kibuon River"), colnames(bgc_data2)))
# Bind the new rows to the existing matrix
#bgc_data2 <- rbind(bgc_data2, new_rows)

#Run NMDS to get points 
set.seed(59)
nmds_all = metaMDS(bgc_data_summed2, distance = "bray")
nmds_all
NMDS_scores_all = as.data.frame(scores(nmds_all)$sites)

plot(nmds_all)

#Create dataframe to use for 
complete_kenya_envfit = complete_kenya2 %>% select(c(site_name_sitenumber, depth, pH, temp, diss_oxygen, conduc, secchi, turbidity, diss_phosp, ammonia, nitrate, cyano_sonde, total_sonde, year, diss_np, LH_classifier))

complete_kenya_envfit2 = complete_kenya_envfit %>% mutate(ammonia = as.character(ammonia)) %>%  # Convert to character type
  mutate(ammonia = if_else(ammonia == "ND", "0", ammonia))

complete_kenya_envfit3 = complete_kenya_envfit2 %>% mutate(depth = as.numeric(depth)) %>% mutate(pH = as.numeric(pH)) %>% mutate(temperature = as.numeric(temp)) %>% select(!temp)  %>% mutate(DO = as.numeric(diss_oxygen)) %>% select(!diss_oxygen)  %>% mutate(conductivity = as.numeric(conduc)) %>% select(!conduc) %>% mutate(turbidity = as.numeric(turbidity)) %>% mutate(ammonia = as.numeric(ammonia)) %>% mutate('dissolved N:P' = diss_np) %>% mutate(cyanos = cyano_sonde) %>% mutate(chlorophyll = total_sonde) %>% mutate('dissolved p' = diss_phosp) %>% mutate(year = as.character(year)) %>% select(!c(diss_phosp, total_sonde, cyano_sonde, diss_np))

str(complete_kenya_envfit3)

#Add main cyano genera rel abundance values to df: microcystis, dolichospermum, planktothrix, cyanobium, vulcanococcus, sphaerospermopsis 
rel_abund_cyano = coverm_cyano_ord2 %>% ungroup() %>% select(c(Genus, site_name_sitenumber, sum_rel_abund_norm))
rel_abund_cyano = rel_abund_cyano %>% pivot_wider(names_from = Genus, values_from = sum_rel_abund_norm)
rel_abund_cyano = rel_abund_cyano %>% select(c(site_name_sitenumber, Dolichospermum, Microcystis, Planktothrix, Sphaerospermopsis, Cyanobium, Vulcanococcus))
#rel_abund_cyano2 = rel_abund_cyano %>% mutate('Dolichospermum*' = Dolichospermum) %>% mutate('Microcystis*' = Microcystis) %>% mutate('Planktothrix*' = Planktothrix) %>% mutate('Sphaerospermopsis*' = Sphaerospermopsis) %>% mutate('Cyanobium*' = Cyanobium) %>% mutate('Vulcanococcus*' = Vulcanococcus)

#Merge with env data set 
complete_kenya_envfit_cyano = left_join(complete_kenya_envfit3, rel_abund_cyano, by = "site_name_sitenumber")

#Exclude rivers sites since empty rows in bgc matrix! 
complete_kenya_envfit_cyano = complete_kenya_envfit_cyano %>% filter(!site_name_sitenumber == "12. Nyando River", !site_name_sitenumber ==  "13. Sondu River Bride", !site_name_sitenumber == "14. Sondu River Shore",!site_name_sitenumber ==  "16. Kibuon River")

#Make dataset of just Rel abund of top 6 cyanos 
complete_kenya_envfit_cyano_only = complete_kenya_envfit_cyano %>% select(c(site_name_sitenumber, LH_classifier, year, Microcystis, Dolichospermum, Sphaerospermopsis, Planktothrix, Cyanobium, Vulcanococcus))


set.seed(59)
vf <- envfit(nmds_all,complete_kenya_envfit_cyano_only, add = TRUE, na.rm = TRUE)

NMDS_vectors <- as.data.frame(scores(vf, display = "vectors"))
NMDS_vectors <- cbind (NMDS_vectors, var = rownames(NMDS_vectors))


NMDS_scores_all$site_name_sitenumber = complete_kenya_envfit_cyano$site_name_sitenumber
NMDS_scores_all$LH_classifier = complete_kenya_envfit_cyano$LH_classifier

NMDS_scores_all$year = complete_kenya_envfit_cyano$year
NMDS_scores_all2 = NMDS_scores_all %>% mutate(year = as.factor(year))

#Add site number 
NMDS_scores_all2$site_number <- as.numeric(gsub("\\D+", "", NMDS_scores_all2$site_name_sitenumber))

#PLOT=Figure SI4

shape_assignments <- c(11, 1, 13, 15)

NMDS_scores_all2$LH_classifier <- factor(NMDS_scores_all2$LH_classifier, 
                                         levels = c("Nearshore", "Offshore", "Near/Outside Rosinga Channel"))

plot = ggplot(NMDS_scores_all2) +
geom_star(mapping = aes(x = NMDS1, y = NMDS2, starshape = LH_classifier, fill = year), size = 7, alpha = 0.8) +
  coord_fixed(ratio = 3) + ## need aspect ratio of 1!
  geom_segment(data = NMDS_vectors,
               aes(x = 0, xend = NMDS1, y = 0, yend = NMDS2),
               arrow = arrow(length = unit(0.25, "cm")), color = "dark grey") +
  geom_text(data = NMDS_vectors, aes(x = NMDS1, y = NMDS2, label = var), size = 5) +
  scale_shape_manual(values = c(15, 1, 11)) + 
  scale_fill_manual(values = c("2022" = "orange", "2023" = "cyan3")) + theme_bw() + 
  theme(panel.background = element_rect(fill = "white"),
        panel.grid = element_blank(),
        panel.grid.major = element_line(color = 'light grey'),
        panel.border = element_rect(color = "black", fill = NA, size = 1),
        axis.text.x = element_text(color = "black", size = 12, face = "bold"),
        axis.text.y = element_text(color = "black", size = 12, face = "bold"),
        axis.title.x = element_text(color = "black", size = 15, face = "bold"),
        axis.title.y = element_text(color = "black", size = 15, face = "bold"),
        legend.text =element_text(size = 12, face = "bold"),
        legend.key = element_rect(fill = NA),
        legend.title = element_text(size = 15, face = "bold")) + 
   labs(starshape = "Station Location", fill = "Year")

plot

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcNMDS_nostatnames_onlycyanos_group.png",plot, width = 6, height =6, dpi=300, scale =2, limitsize = FALSE))

(ggsave("/geomicro/data21/lnhart/Projects/Kenya_2023/flagship_manuscript/FINAL/figures/bgc_mining/bgc_mining_manannot95/2223_bgcNMDS_nostatnames_onlycyanos_group.svg",plot, width = 6, height =6, dpi=300, scale =2, limitsize = FALSE))
```


